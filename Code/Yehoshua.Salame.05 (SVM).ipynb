{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Yehoshua.Salame.05 (SVM).ipynb","provenance":[],"authorship_tag":"ABX9TyPncPAaAPHRx84kiT7Ur6Ry"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tGoiro2AbL4G"},"source":["Note: There are a multitude of print statements and verbosity requests for a better user (me) experience. The prints can be removed obviously; you can modify the verbosity as you please."]},{"cell_type":"code","metadata":{"id":"AJ9WqEFjCmTN"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gPTyaPIUCqtR","outputId":"d23514f5-d857-42c9-97b8-402c6a4fbfa2","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Import MNIST dataset\n","from sklearn.datasets import fetch_openml\n","print(\"Importing dataset\")\n","mnist = fetch_openml('mnist_784', version=1)\n","print(\"Finished importing dataset\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Importing dataset\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YCTpIrxHCvIF"},"source":["X, y = mnist[\"data\"], mnist[\"target\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-yrswOVCzjT"},"source":["# Already split and shuffled, just assign\n","print(\"Splitting into training and test set\")\n","X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n","print(\"Finished splitting into training and test set\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Dk24JpjVP5U"},"source":["# \"global\" variables (don't know best practice for this in python)\n","\n","# verbose set for running time and debugging purposes; change num to 0 and bool to False for code validation. \n","verbose_num=1\n","verbose_bool=True\n","\n","# WARNING: Change n_jobs parameter according to the number of processing cores you want to allocate to the task\n","n_jobs=10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QundArFObwjB"},"source":["SVM binary classifiers can handle multi-class classification automatically with one-vs-rest strategy\n"]},{"cell_type":"markdown","metadata":{"id":"G-PBXSayd80e"},"source":["LinearSVC seems to run into convergence issues with the default tolerance. tol changed to 0.1 and dual to false [because there are more instances than features]"]},{"cell_type":"code","metadata":{"id":"IVhgVSKgDk3G"},"source":["# Pipeline (using LinearSVC for speed on large dataset)\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","\n","print(\"Creating Pipeline\")\n","svm_clf = Pipeline(steps=[\n","    (\"scaler\", StandardScaler()),\n","    (\"LinearSVC\", LinearSVC(dual=False, tol=.1, multi_class=\"ovr\", verbose=verbose_num, random_state=42)),\n","    ], verbose=verbose_bool)\n","\n","print(\"Finished creating Pipeline\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sho-6HtfD0DT"},"source":["# Train on training set\n","print(\"Training\")\n","svm_clf.fit(X_train, y_train)\n","print(\"Finished training\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3snoy-_uFFMW"},"source":["# Cross-validation accuracy (runs fast enough on my computer so I didn't bother making it smaller)\n","from sklearn.model_selection import cross_val_score\n","print(\"Cross-validating\")\n","scores = cross_val_score(svm_clf, X_train, y_train, cv=5, n_jobs=n_jobs, scoring=\"accuracy\", verbose=verbose_num)\n","print(\"Average score: %.2f\" % scores.mean())\n","print(\"Stdev: %.2f\" % scores.std())\n","print(\"Finished cross-validating\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RODrsKheECe6"},"source":["# Predict\n","from sklearn.metrics import accuracy_score\n","print(\"Predicting test set labels\")\n","svm_clf_predictions = svm_clf.predict(X_test)\n","print(\"Finished predicting test set labels\")\n","accuracy = accuracy_score(y_test, svm_clf_predictions)\n","print(\"Accuracy: %.4f\" % accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2BOmaPa2eg4i"},"source":["Turning verbosity off for RandomizedSearch because it isn't helpful."]},{"cell_type":"code","metadata":{"id":"ji9X6-jnEnca"},"source":["# Find best hyperparameters and cross-validate using RandomizedSearchCV\n","# May or may not take forever\n","from sklearn.model_selection import RandomizedSearchCV\n","print(\"Searching for best hyperparameters\")\n","param_dist = [\n","             {'C': [.01, .1, 1, 10, 100]},\n","             {'tol': [0.01, 0.25, 0.50, 0.75, 0.1]},\n","]\n","\n","svm_clf = LinearSVC(dual=False, multi_class=\"ovr\", random_state=42)\n","\n","rnd_search = RandomizedSearchCV(svm_clf, param_rnd, cv=5, n_jobs=n_jobs)\n","rnd_search.fit(X_train, y_train)\n","print(\"Finished search for best hyperparameters\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqR17yUtZeks"},"source":["# Print best hyperparameters and score\n","print(\"Best hyperparameters:\")\n","print(rnd_search.best_params_)\n","print(\"Best score:\")\n","print(rnd_search.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jd-lHLypYmuv"},"source":["# Predict the test set with these hyperparameters\n","from sklearn.metrics import accuracy_score\n","print(\"Starting prediction of test set with best hyperparameters\")\n","rnd_search_predictions = rnd_search.predict(X_test)\n","print(\"Finished prediction of test set\")\n","accuracy = accuracy_score(y_test, rnd_search_predictions)\n","print(\"Accuracy: %.4f\" % accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIOA6_ZKuHEt"},"source":["Locally ran it and got accuracy = 0.9182. Seems low, and is almost certainly underfitting; could be that my hyperparameter ranges are bad (not unlikely, as I did them a magnitude apart each) or a linear classifier just isn't complex enough for this (almost certain). As I recall K-neighbors got around 97%, but took much longer."]}]}
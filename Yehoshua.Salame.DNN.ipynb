{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Yehoshua.Salame.DNN.ipynb","provenance":[{"file_id":"1LRqLb94uE5UuW1TERRXNhF_rU230fGRf","timestamp":1606783125121}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5NtYpEEyxmRA"},"source":["A: Building DNN with 20 layers, 100 neurons each. Use He initialization and ELU activation function."]},{"cell_type":"markdown","metadata":{"id":"n7bBgqUOyHa5"},"source":["B: Train the network on the CIFAR10 dataset.\n","<br>\n","keras.datasets.cifar10.load_data()\n","<br>\n","Dataset is 60k 32 by 32 pixel color images with 10 classes. Use 50k for training and 10k for testing.\n","<br> \n","Use Nadam optimization and Early Stopping. Output layer should have 10 neurons and use softmax."]},{"cell_type":"code","metadata":{"id":"NiCs7y6Svna0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607003546653,"user_tz":300,"elapsed":5657,"user":{"displayName":"Yehoshua S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8lXE6bUl_0TcnqO4NoWnlz4J0Gza_xEKWw8yJ=s64","userId":"05463914661174493943"}},"outputId":"b586b3a9-76c9-4647-8a2e-fb30af58137f"},"source":["# Import TensorFlow and the dataset\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","cifar = keras.datasets.cifar10\n","(X_train, y_train), (X_test, y_test) = cifar.load_data()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L3uSrm2VyVwF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607003546654,"user_tz":300,"elapsed":5649,"user":{"displayName":"Yehoshua S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8lXE6bUl_0TcnqO4NoWnlz4J0Gza_xEKWw8yJ=s64","userId":"05463914661174493943"}},"outputId":"00bb8230-1661-41a1-eb3d-563386d389a9"},"source":["# Dataset properties\n","print(X_train.shape)\n","print(X_train.dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3)\n","uint8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lu3rketE0ddX"},"source":["# Class names\n","class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgddSRoo07Vu"},"source":["# Build classification MLP with twenty hidden layers, 100 neurons each\n","\n","# Create model with Flatten layer, then add 20 dense layers in the next block.\n","model = keras.models.Sequential([ \n","                                 keras.layers.Flatten()\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrCusUXz6c8J"},"source":["# Loop to add the 20 hidden layers. Adds BatchNormalization, Dropout, and Dense layers.\n","for x in range(0, 20) : \n","    model.add(keras.layers.BatchNormalization())\n","    model.add(keras.layers.Dropout(rate=0.2))\n","    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n","\n","# Add output layer using softmax\n","model.add(keras.layers.Dense(10, activation=\"softmax\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJlTnkD0t1V2"},"source":["early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","\n","# Compile with the optimizer (remove nadam variable, it has one pre-built)\n","model.compile(optimizer='nadam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZlAPAyM4l0W"},"source":["X_train = X_train.reshape(-1, 32*32*3)\n","\n","X_train = X_train/255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MdOstgb4jof","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607004664337,"user_tz":300,"elapsed":1123319,"user":{"displayName":"Yehoshua S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8lXE6bUl_0TcnqO4NoWnlz4J0Gza_xEKWw8yJ=s64","userId":"05463914661174493943"}},"outputId":"0ee9a1a9-61e3-4d26-ee30-2121711294a6"},"source":["# Train model\n","history = model.fit(X_train, y_train, epochs=30)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","1563/1563 [==============================] - 39s 25ms/step - loss: 2.2462 - accuracy: 0.1688\n","Epoch 2/30\n","1563/1563 [==============================] - 39s 25ms/step - loss: 1.9823 - accuracy: 0.2522\n","Epoch 3/30\n","1563/1563 [==============================] - 38s 24ms/step - loss: 1.8976 - accuracy: 0.2955\n","Epoch 4/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.8507 - accuracy: 0.3235\n","Epoch 5/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.8088 - accuracy: 0.3430\n","Epoch 6/30\n","1563/1563 [==============================] - 38s 24ms/step - loss: 1.7846 - accuracy: 0.3552\n","Epoch 7/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.7544 - accuracy: 0.3693\n","Epoch 8/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.7418 - accuracy: 0.3747\n","Epoch 9/30\n","1563/1563 [==============================] - 38s 24ms/step - loss: 1.7243 - accuracy: 0.3857\n","Epoch 10/30\n","1563/1563 [==============================] - 38s 24ms/step - loss: 1.7053 - accuracy: 0.3921\n","Epoch 11/30\n","1563/1563 [==============================] - 38s 25ms/step - loss: 1.6916 - accuracy: 0.3993\n","Epoch 12/30\n","1563/1563 [==============================] - 38s 24ms/step - loss: 1.6806 - accuracy: 0.4072\n","Epoch 13/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.6649 - accuracy: 0.4094\n","Epoch 14/30\n","1563/1563 [==============================] - 38s 24ms/step - loss: 1.6602 - accuracy: 0.4145\n","Epoch 15/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.6551 - accuracy: 0.4160\n","Epoch 16/30\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.6436 - accuracy: 0.4171\n","Epoch 17/30\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.6327 - accuracy: 0.4258\n","Epoch 18/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.6209 - accuracy: 0.4260\n","Epoch 19/30\n","1563/1563 [==============================] - 37s 24ms/step - loss: 1.6140 - accuracy: 0.4349\n","Epoch 20/30\n","1563/1563 [==============================] - 37s 23ms/step - loss: 1.6096 - accuracy: 0.4360\n","Epoch 21/30\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.6032 - accuracy: 0.4371\n","Epoch 22/30\n","1563/1563 [==============================] - 35s 23ms/step - loss: 1.5997 - accuracy: 0.4379\n","Epoch 23/30\n","1563/1563 [==============================] - 36s 23ms/step - loss: 1.5914 - accuracy: 0.4416\n","Epoch 24/30\n","1563/1563 [==============================] - 35s 22ms/step - loss: 1.5813 - accuracy: 0.4462\n","Epoch 25/30\n","1563/1563 [==============================] - 35s 23ms/step - loss: 1.5897 - accuracy: 0.4429\n","Epoch 26/30\n","1563/1563 [==============================] - 35s 22ms/step - loss: 1.5755 - accuracy: 0.4472\n","Epoch 27/30\n","1563/1563 [==============================] - 35s 22ms/step - loss: 1.5774 - accuracy: 0.4475\n","Epoch 28/30\n","1563/1563 [==============================] - 35s 22ms/step - loss: 1.5706 - accuracy: 0.4496\n","Epoch 29/30\n","1563/1563 [==============================] - 35s 22ms/step - loss: 1.5738 - accuracy: 0.4506\n","Epoch 30/30\n","1563/1563 [==============================] - 35s 22ms/step - loss: 1.5559 - accuracy: 0.4565\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WFdWSnAAdqu3"},"source":["# Will use tutorial from: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n","# to combat scikit-learn being unable to clone the model object because of it missing the scikit-learn standard get_params method.\n","\n","# Use scikit-learn to rnd search the batch size and epochs\n","import numpy\n","from sklearn.model_selection import RandomizedSearchCV\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.optimizers import Nadam\n","# Function to create model, required for KerasClassifier\n","# Need to figure out how to use early stopping & power scheduling. \n","# Also unsure how I'm going to build the model with one type of dropout, train it, and switch without re-instantiating it.\n","def create_model(batch_normalization=False, dropout_layer=False, dropout_type=None\n","                 activation=\"elu\", kernel_initializer=\"he_normal\", \n","                 early_stopping=True patience=5, \n","                 learn_rate=0.1, power_scheduling=False):\n","    # Create model with Flatten layer, then add 20 dense layers in the next block.\n","    model = keras.models.Sequential([\n","                                     keras.layers.Flatten()\n","    ])\n","    # Build classification MLP with twenty hidden layers, 100 neurons each\n","    for x in range(0, 20) : \n","        if (batch_normalization) :\n","            model.add(keras.layers.BatchNormalization())\n","        if (dropout) :\n","            model.add(keras.layers.Dropout(rate=0.2))\n","        model.add(keras.layers.Dense(100, activation=activation, kernel_initializer=kernel_initializer))\n","\t# Early Stopping\n","    if (early_stopping) :\n","        early_stopping = keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True)\n","\n","    # Compile model\n","    optimizer = Nadam(lr=learn_rate)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"1E_80YSSdqr-","executionInfo":{"status":"error","timestamp":1607013186282,"user_tz":300,"elapsed":17262,"user":{"displayName":"Yehoshua S.","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8lXE6bUl_0TcnqO4NoWnlz4J0Gza_xEKWw8yJ=s64","userId":"05463914661174493943"}},"outputId":"0deff64a-8fe0-4b0b-bdb1-74d00c1789ac"},"source":["# GridSearchCV for learning rate optimization\n","# Create model using the KerasClassifier wrapper\n","model = KerasClassifier(build_fn=create_model, verbose=1)\n","\n","# Define the rnd search parameters, pass them in, and train.\n","epochs = 50\n","learn_rate = [0.01, 0.05, 0.1, 0.2, 0.3]\n","param_dist = dict(learn_rate=learn_rate)\n","rnd = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=5, n_jobs=-1, cv=3, verbose=1, random_state=42)\n","rnd_result = rnd.fit(X_train, y_train)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n","means = rnd_result.cv_results_['mean_test_score']\n","stds = rnd_result.cv_results_['std_test_score']\n","params = rnd_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 5 is smaller than n_iter=9. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n","  % (grid_size, self.n_iter, grid_size), UserWarning)\n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-b1b2a294ca5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mparam_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrnd_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m             random_state=self.random_state))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"fYp-kqbaTfx5"},"source":["# Save best parameters from previous step (except for learn_rate, which we will rnd search for after adding BatchNormalization. \n","# Running all hyperparamers again would be ideal but unrealistic)\n","\n","epochs = params[0]\n","# printing to confirm I assigned it right\n","print(epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1FClCIY7VL0"},"source":["# Try with Batch Normalization\n","# Considering naming the models appropriately instead of overwriting my previous one (model_BN for example)\n","\n","# Create model using the KerasClassifier wrapper\n","model = KerasClassifier(build_fn=create_model, verbose=1)\n","\n","# Define the rnd search parameters, pass them in, and train.\n","learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n","param_dist = dict(batch_size=batch_size, epochs=epochs, batch_normalization=True, learn_rate=learn_rate)\n","rnd = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=30, n_jobs=-1, cv=3, verbose=1, random_state=42)\n","rnd_result = rnd.fit(X_train, y_train)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n","means = rnd_result.cv_results_['mean_test_score']\n","stds = rnd_result.cv_results_['std_test_score']\n","params = rnd_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZWgpl3xbNOp"},"source":["# Try with SELU instead of Batch Normalization\n","\n","# Create model using the KerasClassifier wrapper\n","model = KerasClassifier(build_fn=create_model, verbose=1)\n","\n","# Define the rnd search parameters, pass them in, and train.\n","learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n","param_dist = dict(batch_size=batch_size, epochs=epochs, activation=\"selu\", kernel_initializer=\"lecun_normal\", learn_rate=learn_rate)\n","rnd = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=30, n_jobs=-1, cv=3, verbose=1, random_state=42)\n","rnd_result = rnd.fit(X_train, y_train)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n","means = rnd_result.cv_results_['mean_test_score']\n","stds = rnd_result.cv_results_['std_test_score']\n","params = rnd_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"We8x74tEbWHi"},"source":["# Try with alpha dropout\n","dropout_layer=True\n","dropout_type=\"alpha\" #check docss for name\n","\n","# Create model using the KerasClassifier wrapper\n","model = KerasClassifier(build_fn=create_model, verbose=1)\n","\n","# Define the rmd search parameters, pass them in, and train.\n","learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n","param_dist = dict(batch_size=batch_size, epochs=epochs, activation=\"selu\", kernel_initializer=\"lecun_normal\", learn_rate=learn_rate)\n","rnd = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=30, n_jobs=-1, cv=3, verbose=1, random_state=42)\n","rnd_result = rnd.fit(X_train, y_train)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n","means = rnd_result.cv_results_['mean_test_score']\n","stds = rnd_result.cv_results_['std_test_score']\n","params = rnd_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mf3Cx0u0bWCe"},"source":["# Try with MC Dropout instead of alpha dropout (unsure how to do this without recreating the model, considering its made with the dropout layers configured)\n","dropout_layer=True\n","dropout_type=\"mc\" #check docs for name\n","\n","# Create model using the KerasClassifier wrapper\n","model = KerasClassifier(build_fn=create_model, verbose=1)\n","\n","# Define the grid search parameters, pass them in, and train.\n","batch_normalization=False\n","\n","learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n","param_dist = dict(batch_size=batch_size, epochs=epochs, activation=\"selu\", kernel_initializer=\"lecun_normal\", learn_rate=learn_rate)\n","rnd = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=30, n_jobs=-1, cv=3, verbose=1, random_state=42)\n","rnd_result = rnd.fit(X_train, y_train)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n","means = rnd_result.cv_results_['mean_test_score']\n","stds = rnd_result.cv_results_['std_test_score']\n","params = rnd_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9XX30G7bV8d"},"source":["# Try with power scheduling\n"],"execution_count":null,"outputs":[]}]}